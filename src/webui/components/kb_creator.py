# -*- coding: utf-8 -*-
"""
çŸ¥è¯†åº“åˆ›å»ºç»„ä»¶
"""
import streamlit as st
from pathlib import Path
import yaml


class KnowledgeBaseCreator:
    """çŸ¥è¯†åº“åˆ›å»ºç»„ä»¶"""

    def __init__(self, kb_manager):
        self.kb_manager = kb_manager
        self._vector_store_options = None
        self._embedder_options = None

    def _get_vector_store_options(self):
        """è·å–å‘é‡å­˜å‚¨é€‰é¡¹"""
        if self._vector_store_options is None:
            try:
                import requests
                from .. import API_BASE_URL

                response = requests.get(f"{API_BASE_URL}/vector-stores/list", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    self._vector_store_options = data.get("vector_stores", [])
                else:
                    # APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹
                    self._vector_store_options = [
                        {"type": "chroma", "name": "ChromaDB (æ¨è)"},
                        {"type": "faiss", "name": "FAISS (æœ¬åœ°)"},
                        {"type": "milvus", "name": "Milvus (ç”Ÿäº§)"}
                    ]
            except Exception as e:
                # ç½‘ç»œé”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹
                self._vector_store_options = [
                    {"type": "chroma", "name": "ChromaDB (æ¨è)"},
                    {"type": "faiss", "name": "FAISS (æœ¬åœ°)"},
                    {"type": "milvus", "name": "Milvus (ç”Ÿäº§)"}
                ]
        return self._vector_store_options

    def _get_embedder_options(self):
        """è·å–åµŒå…¥å™¨é€‰é¡¹"""
        if self._embedder_options is None:
            try:
                import requests
                from .. import API_BASE_URL

                response = requests.get(f"{API_BASE_URL}/embedders/list", timeout=5)
                if response.status_code == 200:
                    data = response.json()
                    self._embedder_options = data.get("embedders", [])
                else:
                    # APIè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹
                    self._embedder_options = [
                        {
                            "type": "bge",
                            "name": "BGEä¸­æ–‡æ¨¡å‹ (æ¨è)",
                            "models": [
                                {"name": "BAAI/bge-small-zh-v1.5", "description": "å°å‹ä¸­æ–‡æ¨¡å‹"},
                                {"name": "BAAI/bge-base-zh-v1.5", "description": "åŸºç¡€ä¸­æ–‡æ¨¡å‹"},
                                {"name": "BAAI/bge-large-zh-v1.5", "description": "å¤§å‹ä¸­æ–‡æ¨¡å‹"}
                            ]
                        },
                        {
                            "type": "openai",
                            "name": "OpenAI Embeddings",
                            "models": [
                                {"name": "text-embedding-3-small", "description": "å°å‹æ¨¡å‹"},
                                {"name": "text-embedding-3-large", "description": "å¤§å‹æ¨¡å‹"},
                                {"name": "text-embedding-ada-002", "description": "ç»å…¸æ¨¡å‹"}
                            ]
                        },
                        {
                            "type": "local",
                            "name": "æœ¬åœ° Sentence Transformers",
                            "models": [
                                {"name": "sentence-transformers/all-MiniLM-L6-v2", "description": "é€šç”¨å°å‹æ¨¡å‹"},
                                {"name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                                 "description": "å¤šè¯­è¨€æ¨¡å‹"}
                            ]
                        }
                    ]
            except Exception as e:
                # ç½‘ç»œé”™è¯¯ï¼Œä½¿ç”¨é»˜è®¤é€‰é¡¹
                self._embedder_options = [
                    {
                        "type": "bge",
                        "name": "BGEä¸­æ–‡æ¨¡å‹ (æ¨è)",
                        "models": [
                            {"name": "BAAI/bge-small-zh-v1.5", "description": "å°å‹ä¸­æ–‡æ¨¡å‹"},
                            {"name": "BAAI/bge-base-zh-v1.5", "description": "åŸºç¡€ä¸­æ–‡æ¨¡å‹"},
                            {"name": "BAAI/bge-large-zh-v1.5", "description": "å¤§å‹ä¸­æ–‡æ¨¡å‹"}
                        ]
                    },
                    {
                        "type": "openai",
                        "name": "OpenAI Embeddings",
                        "models": [
                            {"name": "text-embedding-3-small", "description": "å°å‹æ¨¡å‹"},
                            {"name": "text-embedding-3-large", "description": "å¤§å‹æ¨¡å‹"},
                            {"name": "text-embedding-ada-002", "description": "ç»å…¸æ¨¡å‹"}
                        ]
                    },
                    {
                        "type": "local",
                        "name": "æœ¬åœ° Sentence Transformers",
                        "models": [
                            {"name": "sentence-transformers/all-MiniLM-L6-v2", "description": "é€šç”¨å°å‹æ¨¡å‹"},
                            {"name": "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                             "description": "å¤šè¯­è¨€æ¨¡å‹"}
                        ]
                    }
                ]
        return self._embedder_options

    def render(self):
        """æ¸²æŸ“åˆ›å»ºçŸ¥è¯†åº“é¡µé¢"""
        st.subheader("ğŸš€ åˆ›å»ºæ–°çŸ¥è¯†åº“")

        # çŸ¥è¯†åº“åŸºæœ¬ä¿¡æ¯
        col1, col2 = st.columns(2)
        with col1:
            kb_name = st.text_input("çŸ¥è¯†åº“åç§°", value="my_knowledge_base")
        with col2:
            kb_description = st.text_input("æè¿°", value="æˆ‘çš„çŸ¥è¯†åº“")

        # å‘é‡å­˜å‚¨é…ç½®
        self._render_vector_config()

        # åµŒå…¥æ¨¡å‹é…ç½®
        self._render_embedder_config()

        # æ–‡æœ¬å¤„ç†é…ç½®
        self._render_text_config()

        # åˆ›å»ºæŒ‰é’®
        self._render_create_button(kb_name, kb_description)

    def _render_vector_config(self):
        """æ¸²æŸ“å‘é‡å­˜å‚¨é…ç½®"""
        st.subheader("ğŸ’¾ å‘é‡å­˜å‚¨é…ç½®")

        vector_config_col1, vector_config_col2 = st.columns(2)

        with vector_config_col1:
            # è·å–å‘é‡å­˜å‚¨ç±»å‹åˆ—è¡¨
            vector_store_options = self._get_vector_store_options()
            vector_store_type = st.selectbox(
                "å‘é‡æ•°æ®åº“",
                options=[opt["type"] for opt in vector_store_options],
                format_func=lambda x: next((opt["name"] for opt in vector_store_options if opt["type"] == x), x)
            )

        with vector_config_col2:
            # å‘é‡å­˜å‚¨ç‰¹å®šé…ç½®
            if vector_store_type == "chroma":
                persist_dir = st.text_input(
                    "æŒä¹…åŒ–ç›®å½•",
                    value=f"./data/vector_stores/{st.session_state.get('kb_name', 'kb')}"
                )
                collection_name = st.text_input("é›†åˆåç§°", value=st.session_state.get('kb_name', 'kb'))

            elif vector_store_type == "faiss":
                index_type = st.selectbox(
                    "ç´¢å¼•ç±»å‹",
                    ["Flat", "IVF", "HNSW"],
                    help="FAISSç´¢å¼•ç®—æ³•"
                )
                nlist = st.number_input("èšç±»æ•°é‡", min_value=1, max_value=10000,
                                        value=100) if index_type == "IVF" else None

            elif vector_store_type == "milvus":
                host = st.text_input("Milvusåœ°å€", value="localhost")
                port = st.number_input("ç«¯å£", min_value=1, max_value=65535, value=19530)
                collection_name = st.text_input("é›†åˆåç§°", value=st.session_state.get('kb_name', 'kb'))

        # å­˜å‚¨é…ç½®åˆ°session state
        st.session_state.vector_config = {
            'store_type': vector_store_type,
            'persist_dir': locals().get('persist_dir'),
            'collection_name': locals().get('collection_name'),
            'host': locals().get('host'),
            'port': locals().get('port'),
            'index_type': locals().get('index_type'),
            'nlist': locals().get('nlist')
        }

    def _render_embedder_config(self):
        """æ¸²æŸ“åµŒå…¥æ¨¡å‹é…ç½®"""
        st.subheader("ğŸ§  åµŒå…¥æ¨¡å‹é…ç½®")

        embed_config_col1, embed_config_col2 = st.columns(2)

        with embed_config_col1:
            # è·å–åµŒå…¥å™¨ç±»å‹é€‰é¡¹
            embedder_options = self._get_embedder_options()
            embedder_type = st.selectbox(
                "åµŒå…¥æ¨¡å‹ç±»å‹",
                options=[opt["type"] for opt in embedder_options],
                format_func=lambda x: next((opt["name"] for opt in embedder_options if opt["type"] == x), x),
                help="é€‰æ‹©è¦ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹ç±»å‹"
            )

            # è·å–å½“å‰é€‰ä¸­åµŒå…¥å™¨çš„æ¨¡å‹åˆ—è¡¨
            embedder_options = self._get_embedder_options()
            current_embedder = next((opt for opt in embedder_options if opt["type"] == embedder_type), None)

            if current_embedder and "models" in current_embedder:
                # ä»é…ç½®ä¸­è·å–æ¨¡å‹é€‰é¡¹
                model_options = current_embedder["models"]
                model_names = [model["name"] for model in model_options]

                # æ·»åŠ è‡ªå®šä¹‰é€‰é¡¹ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if embedder_type == "local":
                    model_names.append("è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„")

                model_name = st.selectbox(
                    "æ¨¡å‹",
                    model_names,
                    format_func=lambda x: next(
                        (f'{model["name"]} - {model["description"]}' for model in model_options if model["name"] == x),
                        x),
                    help="é€‰æ‹©è¦ä½¿ç”¨çš„å…·ä½“æ¨¡å‹"
                )

                # å¦‚æœæ˜¯è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„ï¼Œæ˜¾ç¤ºè¾“å…¥æ¡†
                if model_name == "è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„":
                    model_name = st.text_input("æ¨¡å‹è·¯å¾„")

                # æ˜¾ç¤ºæ¨¡å‹ç»´åº¦ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                selected_model_info = next((model for model in model_options if model["name"] == model_name), None)
                if selected_model_info and "dimensions" in selected_model_info:
                    st.info(f"ğŸ“ å‘é‡ç»´åº¦: {selected_model_info['dimensions']}")

            else:
                # å›é€€åˆ°é»˜è®¤è¡Œä¸ºï¼ˆå¦‚æœAPIæ•°æ®ä¸å®Œæ•´ï¼‰
                if embedder_type == "openai":
                    model_name = st.selectbox("æ¨¡å‹", ["text-embedding-3-small", "text-embedding-3-large",
                                                       "text-embedding-ada-002"])
                    dimensions = st.number_input("ç»´åº¦", min_value=256, max_value=3072, value=1536)
                elif embedder_type == "local":
                    model_name = st.selectbox("æ¨¡å‹åç§°", ["sentence-transformers/all-MiniLM-L6-v2",
                                                           "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
                                                           "è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„"])
                    if model_name == "è‡ªå®šä¹‰æ¨¡å‹è·¯å¾„":
                        model_name = st.text_input("æ¨¡å‹è·¯å¾„")
                elif embedder_type == "bge":
                    model_name = st.selectbox("BGEæ¨¡å‹", ["BAAI/bge-small-zh-v1.5", "BAAI/bge-base-zh-v1.5",
                                                          "BAAI/bge-large-zh-v1.5"])
                    normalize_embeddings = st.checkbox("å½’ä¸€åŒ–å‘é‡", value=True)

            # OpenAIç‰¹æœ‰çš„é…ç½®
            if embedder_type == "openai":
                openai_key = st.text_input("OpenAI API Key", type="password")

            # BGEç‰¹æœ‰çš„é…ç½®
            if embedder_type == "bge":
                normalize_embeddings = st.checkbox("å½’ä¸€åŒ–å‘é‡", value=True)

        with embed_config_col2:
            # é€šç”¨åµŒå…¥é…ç½®
            batch_size = st.number_input(
                "æ‰¹å¤„ç†å¤§å°",
                min_value=1,
                max_value=1000,
                value=32,
                help="æ‰¹é‡å¤„ç†æ–‡æœ¬çš„æ•°é‡"
            )
            device = st.selectbox(
                "è¿è¡Œè®¾å¤‡",
                ["auto", "cpu", "cuda"],
                help="æ¨¡å‹è¿è¡Œè®¾å¤‡ï¼Œautoä¸ºè‡ªåŠ¨é€‰æ‹©"
            )

        # å­˜å‚¨é…ç½®åˆ°session state
        st.session_state.embedder_config = {
            'embedder_type': embedder_type,
            'model_name': locals().get('model_name'),
            'openai_key': locals().get('openai_key'),
            'dimensions': locals().get('dimensions'),
            'normalize_embeddings': locals().get('normalize_embeddings'),
            'batch_size': batch_size,
            'device': device
        }

    def _render_text_config(self):
        """æ¸²æŸ“æ–‡æœ¬å¤„ç†é…ç½®"""
        st.subheader("ğŸ“ æ–‡æœ¬å¤„ç†é…ç½®")

        text_config_col1, text_config_col2 = st.columns(2)

        with text_config_col1:
            # åˆ†å‰²å™¨é…ç½®
            splitter_type = st.selectbox(
                "åˆ†å‰²å™¨ç±»å‹",
                ["recursive", "semantic", "fixed"],
                format_func=lambda x: {
                    "recursive": "é€’å½’åˆ†å‰² (æ¨è)",
                    "semantic": "è¯­ä¹‰åˆ†å‰²",
                    "fixed": "å›ºå®šé•¿åº¦åˆ†å‰²"
                }[x]
            )

            chunk_size = st.number_input(
                "åˆ†å—å¤§å°",
                min_value=100,
                max_value=2000,
                value=500,
                help="æ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°"
            )

        with text_config_col2:
            chunk_overlap = st.number_input(
                "é‡å å¤§å°",
                min_value=0,
                max_value=500,
                value=50,
                help="ç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°"
            )

            if splitter_type == "semantic":
                semantic_threshold = st.slider(
                    "è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.5,
                    help="å¥å­åˆå¹¶çš„ç›¸ä¼¼åº¦é˜ˆå€¼"
                )
                semantic_model = st.selectbox(
                    "è¯­ä¹‰åˆ†å‰²æ¨¡å‹",
                    ["paraphrase-multilingual-MiniLM-L12-v2", "all-MiniLM-L6-v2"]
                )

        # å­˜å‚¨é…ç½®åˆ°session state
        st.session_state.text_config = {
            'splitter_type': splitter_type,
            'chunk_size': chunk_size,
            'chunk_overlap': chunk_overlap,
            'semantic_threshold': locals().get('semantic_threshold'),
            'semantic_model': locals().get('semantic_model')
        }

    def _render_create_button(self, kb_name, kb_description):
        """æ¸²æŸ“åˆ›å»ºæŒ‰é’®"""
        st.divider()

        if st.button("ğŸš€ åˆ›å»ºçŸ¥è¯†åº“", type="primary"):
            with st.spinner("æ­£åœ¨åˆ›å»ºçŸ¥è¯†åº“..."):
                try:
                    # æ„å»ºé…ç½®
                    kb_config = self._build_kb_config(kb_name, kb_description)

                    # è°ƒç”¨APIåˆ›å»ºç©ºçš„çŸ¥è¯†åº“
                    import requests
                    from .. import API_BASE_URL

                    payload = {
                        "kb_name": kb_name,
                        "chunk_size": kb_config["chunk_size"],
                        "chunk_overlap": kb_config["chunk_overlap"]
                        # æ³¨æ„ï¼šä¸åŒ…å«file_pathsï¼Œåˆ›å»ºç©ºçš„çŸ¥è¯†åº“
                    }

                    response = requests.post(f"{API_BASE_URL}/knowledge_base/create", json=payload, timeout=60)

                    if response.status_code == 200:
                        result = response.json()

                        # æ˜¾ç¤ºç»“æœ
                        st.success("ğŸ‰ çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸï¼")
                        st.info("ğŸ’¡ çŸ¥è¯†åº“å·²åˆ›å»ºå®Œæˆï¼Œæ‚¨å¯ä»¥åœ¨'ä¸Šä¼ æ–‡ä»¶'é¡µé¢ä¸­æ·»åŠ æ–‡æ¡£ã€‚")

                        # æ˜¾ç¤ºçŸ¥è¯†åº“ä¿¡æ¯
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("çŸ¥è¯†åº“åç§°", result["kb_name"])
                        with col2:
                            st.metric("åˆå§‹æ–‡æ¡£æ•°", result["document_count"])

                        # ä¿å­˜é…ç½®
                        self._save_kb_config(kb_config)

                        # åˆ·æ–°çŸ¥è¯†åº“åˆ—è¡¨
                        st.rerun()

                    else:
                        st.error(f"åˆ›å»ºå¤±è´¥ (çŠ¶æ€ç : {response.status_code})")
                        st.caption(f"é”™è¯¯è¯¦æƒ…: {response.text}")

                except requests.exceptions.Timeout:
                    st.error("â° åˆ›å»ºè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")
                except requests.exceptions.ConnectionError:
                    st.error("ğŸŒ ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦è¿è¡Œ")
                except Exception as e:
                    st.error(f"âŒ åˆ›å»ºå‡ºé”™: {str(e)}")
                    st.caption("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è”ç³»ç®¡ç†å‘˜")

    def _build_kb_config(self, kb_name, kb_description):
        """æ„å»ºçŸ¥è¯†åº“é…ç½®"""
        vector_config = st.session_state.get('vector_config', {})
        embedder_config = st.session_state.get('embedder_config', {})
        text_config = st.session_state.get('text_config', {})

        kb_config = {
            "name": kb_name,
            "description": kb_description,
            "splitter_type": text_config.get('splitter_type', 'recursive'),
            "chunk_size": text_config.get('chunk_size', 500),
            "chunk_overlap": text_config.get('chunk_overlap', 50),
            "embedder": {
                "embedder_type": embedder_config.get('embedder_type', 'bge'),
                "model": embedder_config.get('model_name', 'BAAI/bge-small-zh-v1.5'),
                "dimensions": embedder_config.get('dimensions'),
                "normalize_embeddings": embedder_config.get('normalize_embeddings'),
                "device": embedder_config.get('device', 'auto')
            },
            "vector_store": {
                "store_type": vector_config.get('store_type', 'chroma'),
                "collection_name": vector_config.get('collection_name', kb_name),
                "persist_directory": vector_config.get('persist_dir', f"./data/vector_stores/{kb_name}"),
                "host": vector_config.get('host'),
                "port": vector_config.get('port')
            }
        }

        # æ·»åŠ è¯­ä¹‰åˆ†å‰²ç‰¹å®šé…ç½®
        if text_config.get('splitter_type') == "semantic":
            kb_config["semantic_threshold"] = text_config.get('semantic_threshold', 0.5)
            kb_config["semantic_model"] = text_config.get('semantic_model', 'paraphrase-multilingual-MiniLM-L12-v2')

        return kb_config

    def _save_kb_config(self, kb_config):
        """ä¿å­˜çŸ¥è¯†åº“é…ç½®"""
        config_dir = Path("./configs/knowledge_bases")
        config_dir.mkdir(parents=True, exist_ok=True)
        config_file = config_dir / f"{kb_config['name']}.yaml"

        with open(config_file, "w", encoding="utf-8") as f:
            yaml.dump(kb_config, f, default_flow_style=False, allow_unicode=True)

        st.info(f"é…ç½®æ–‡ä»¶å·²ä¿å­˜: {config_file}")
